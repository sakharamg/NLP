{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final WSD Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITTFGLqxfAw6",
        "outputId": "93e3e7dd-7acb-4059-ecca-a0f80b00383e"
      },
      "source": [
        "!pip3 install nltk==3.6.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idQu8C1sOzEL"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib\n",
        "import nltk\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "def generate_pos_tags(test_sentence):\n",
        "               \n",
        "\t\tfrom nltk.corpus import brown\n",
        "\n",
        "\n",
        "\n",
        "\t\t#Use universal tagset , currently not using\n",
        "\t\tsentence_tag = nltk.corpus.brown.tagged_sents(tagset=\"universal\")\n",
        "\t\tmodified_sentence_tag=[]\n",
        "\t\tfor sent in sentence_tag:\n",
        "\t\t\tsent.insert(0,('^^','^^'))         # Sentence starts with '^^'\n",
        "\t\t\tsent.append(('$$','$$'))           # Sentence ends with '$$'\n",
        "\t\t\tmodified_sentence_tag.append(sent)\n",
        "\n",
        "\n",
        "\t\ttrain_dataset = modified_sentence_tag\n",
        "\n",
        "\t\tmodified_test_sentence = '^^ '+ test_sentence + ' $$'\n",
        "\n",
        "\t\t#needs to be created\n",
        "\t\ttest_dataset = modified_test_sentence.split(' ')\n",
        "\n",
        "\t\t#Creation of a dictionary whose keys are tags and values contain words which have correspoding tag in the taining dataset\n",
        "\t\t#example:- 'TAG':{word1: count(word1,'TAG')} count(word1,'TAG') means how many times the word is tagged as 'TAG'\n",
        "\t\ttrain_word_tag = {}\n",
        "\t\tfor sent in train_dataset:\n",
        "\t\t\tfor (word,tag) in sent:\n",
        "\t\t\t\tword=word.lower()            # removing ambiguity from capital letters \n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\ttrain_word_tag[tag][word]+=1\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\ttrain_word_tag[tag][word]=1\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\ttrain_word_tag[tag]={word:1}\n",
        "\n",
        "\n",
        "\t\t#Calculation of emission probabilities using train_word_tag\n",
        "\t\ttrain_emission_prob={}\n",
        "\t\tfor key in train_word_tag.keys():\n",
        "\t\t\ttrain_emission_prob[key]={}\n",
        "\t\t\tcount = sum(train_word_tag[key].values())                           # count is total number of words tagged as a 'TAG'\n",
        "\t\t\tfor key2 in train_word_tag[key].keys():\n",
        "\t\t\t\ttrain_emission_prob[key][key2]=train_word_tag[key][key2]/count    \n",
        "\n",
        "\t\t#Emission probability is #times a word occured as 'TAG' / total number of 'TAG' words\n",
        "\t\t#example: number of times 'Sandeep' occured as Noun / total number of nouns\n",
        "\n",
        "\n",
        "\t\t#Estimating the bigrams of tags to be used for calculation of transition probability \n",
        "\t\t#Bigram Assumption is made, the current tag depends only on the previous tag\n",
        "\t\tbigram_tag_data = {}\n",
        "\t\tfor sent in train_dataset:\n",
        "\t\t\tbi=list(nltk.bigrams(sent))\n",
        "\t\t\tfor b1,b2 in bi:\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\tbigram_tag_data[b1[1]][b2[1]]+=1\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\tbigram_tag_data[b1[1]][b2[1]]=1\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tbigram_tag_data[b1[1]]={b2[1]:1}\n",
        "\t\t#bigram_tag_data is storing the values for every tag.\n",
        "\t\t#Every key is a tag and value is tag followed for that key and corresponding counts.\n",
        "\t\t#example: how many times an adj is followed by a noun {Noun:{Adj:3}}, here its 3 times.\n",
        "\n",
        "\t\t#Calculation of the probabilities of tag bigrams for transition probability\n",
        "\t\t#We already made a bigram assumption   \n",
        "\t\tbigram_tag_prob={}\n",
        "\t\tfor key in bigram_tag_data.keys():\n",
        "\t\t\tbigram_tag_prob[key]={}\n",
        "\t\t\tcount=sum(bigram_tag_data[key].values())              # count is total number of times a 'TAG' has occured\n",
        "\t\t\tfor key2 in bigram_tag_data[key].keys():\n",
        "\t\t\t\tbigram_tag_prob[key][key2]=bigram_tag_data[key][key2]/count\n",
        "\t\t#Tranmission probability is #times a TAG2 is preceded by TAG1 / total number of times TAG1 exists in dataset\n",
        "\t\t#example: number of times a noun occured before adjective / total number of times a noun occurred\n",
        "\n",
        "\n",
        "\t\t#Calculation the possible tags for each word in the entire dataset\n",
        "\t\t#Note: Here we have used the whole data(Train dataset + Test dataset)\n",
        "\t\t#Reason: Words present in Test data is not subset pf Train data.\n",
        "\t\t#The above thing can be neglected if not necessay, but it improves our accuracy of the model \n",
        "\t\ttags_of_tokens = {}\n",
        "\t\tcount=0\n",
        "\t\tfor sent in train_dataset:\n",
        "\t\t\tfor (word,tag) in sent:\n",
        "\t\t\t\tword=word.lower()\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tif tag not in tags_of_tokens[word]:\n",
        "\t\t\t\t\t\ttags_of_tokens[word].append(tag)\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tlist_of_tags = []\n",
        "\t\t\t\t\tlist_of_tags.append(tag)\n",
        "\t\t\t\t\ttags_of_tokens[word] = list_of_tags\n",
        "\t\t#Each word and its corresponding tags in the train dataset\n",
        "\n",
        "\n",
        "\n",
        "\t\t#Seperating the test data into test words\n",
        "\t\ttest_words= [test_dataset]\n",
        "\t\t# print(test_words)\n",
        "\n",
        "\t\t\n",
        "\t\tpredicted_tags = []                #Final list for prediction\n",
        "\t\tfor i in range(len(test_words)):   # for each tokenized sentence in the test data (test_words is a list of lists)\n",
        "\t\t\tsent = test_words[i]\n",
        "\t\t\t#storing_values is a dictionary which stores the required values\n",
        "\t\t\t#ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
        "\t\t\tstoring_values = {}              \n",
        "\t\t\tfor q in range(len(sent)):\n",
        "\t\t\t\tstep = sent[q]\n",
        "\t\t\t\t#for the starting word of the sentence\n",
        "\t\t\t\tif q == 1:                \n",
        "\t\t\t\t\tstoring_values[q] = {}\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\ttags = tags_of_tokens[step]\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t# print(step,test_tags_of_tokens[step])\n",
        "\t\t\t\t\t\ttags=['NOUN'] #tags_of_unseen_tokens\n",
        "\t\t\t\t\tfor t in tags:\n",
        "\t\t\t\t\t\t#this is applied since we do not know whether the word in the test data is present in train data or not\n",
        "\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\tstoring_values[q][t] = ['^^',bigram_tag_prob['^^'][t]*train_emission_prob[t][step]]\n",
        "\t\t\t\t\t\t\t#if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
        "\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\tstoring_values[q][t] = ['^^',0.0001]\n",
        "\t\t  \n",
        "\t\t\t\t  #if the word is not at the start of the sentence\n",
        "\t\t\t\tif q>1:\n",
        "\t\t\t\t\tstoring_values[q] = {}\n",
        "\t\t\t\t\tprevious_states = list(storing_values[q-1].keys())   # loading the previous states\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\tcurrent_states  = tags_of_tokens[step]               # loading the current states\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\tcurrent_states = ['NOUN']#tags_of_unseen_tokens\n",
        "\t\t\t\t\t\t#calculation of the best previous state for each current state and then storing\n",
        "\t\t\t\t\t\t#it in storing_values\n",
        "\t\t\t\t\tfor t in current_states:                             \n",
        "\t\t\t\t\t\ttemp = []\n",
        "\t\t\t\t\t\tfor pt in previous_states:                         \n",
        "\t\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\t  temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step]) # If seen word\n",
        "\t\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\t  temp.append(storing_values[q-1][pt][1]*0.0001)\n",
        "\t\t\t\t\t\tmax_temp_index = temp.index(max(temp))\n",
        "\t\t\t\t\t\tbest_pt = previous_states[max_temp_index]\n",
        "\t\t\t\t\t\tstoring_values[q][t]=[best_pt,max(temp)] #Store the best previous tag for each best candidate per tag and the meximum probability\n",
        "\t\t \n",
        "\t\t\t#Backtracing to extract the best possible tags for the sentence\n",
        "\t\t\t# for each word looking the current word and the word and tag next to it in the sentence backtrack\n",
        "\t\t\t# to get the tag of current word\n",
        "\t\t\tpred_tags = [] #predicted tags by viterbi using backtracking\n",
        "\t\t\ttotal_steps_num = storing_values.keys()\n",
        "\t\t\tlast_step_num = max(total_steps_num)     # Begin from the last word which will end the delimiter\n",
        "\t\t\tfor bs in range(len(total_steps_num)):    \n",
        "\t\t\t\tstep_num = last_step_num - bs          \n",
        "\t\t\t\tif step_num == last_step_num:\n",
        "\t\t\t\t\tpred_tags.append('$$')\n",
        "\t\t\t\t\tpred_tags.append(storing_values[step_num]['$$'][0]) \n",
        "\t\t\t\tif step_num<last_step_num and step_num>0:\n",
        "\t\t\t\t\tpred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0]) #Looking into storing value fetch the best previous tag for the current word\n",
        "\t\t\tpredicted_tags.append(list(reversed(pred_tags)))\n",
        "\t\treturn predicted_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUXnj6GjeR07",
        "outputId": "42380e2b-c249-47ee-cb3c-c6010acdc524"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import semcor\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Getting the brown corpus\n",
        "print(\"Downloading files from NLTK please wait...\")\n",
        "nltk.download('all', quiet=True)\n",
        "print(\"NLTK files downloaded!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from NLTK please wait...\n",
            "NLTK files downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IhWjMfu0sX0"
      },
      "source": [
        "synCorpus=[]\n",
        "current_index=0\n",
        "for taggedSents in semcor.tagged_sents(tag='sem'): # for each sentence fetch the (phrase,synsetTree)\n",
        "    synCorpus.append([])                           # create a list designating current sentence which will contain (phrase,synsetID)\n",
        "    for phrase in taggedSents:\n",
        "        if type(phrase)==list:                     # The phrase with no tag is a list in semcor [Note: CASE 1]\n",
        "            pass\n",
        "        else: \n",
        "            try: \n",
        "                # handle Case 2\n",
        "                synCorpus[current_index].append((phrase.label().name(),phrase.label().synset().name())) \n",
        "            except:\n",
        "                # handle Case 3, The tags with no synset id but a synset present\n",
        "                pass\n",
        "    current_index+=1 # Keeps track of the sentence index\n",
        "no_sents=current_index #Saved total number of sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njudl5KOfjmy",
        "outputId": "b2d22e71-e80b-4d93-c3c6-cb4cf5220b1b"
      },
      "source": [
        "print(\"Loading word2vec\")\n",
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "try:\n",
        "\tmodel_w2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
        "except:\n",
        "\tprint(\"Download pretrained word2vec from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz and save in the same directory of the file\\nExiting...\")\n",
        "\texit()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading word2vec\n",
            "--2021-11-03 18:40:13--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.227.152\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.227.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.4MB/s    in 1m 53s  \n",
            "\n",
            "2021-11-03 18:42:06 (14.0 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UibfEuBfjh8"
      },
      "source": [
        "TEST FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za1Jla7hzuFq"
      },
      "source": [
        "def test(sentence):\n",
        "  one_sentence=[]\n",
        "  text = nltk.word_tokenize(sentence)\n",
        "  sentence_tags=nltk.pos_tag(text)\n",
        "  sentence_list=[]\n",
        "  for word in text:\n",
        "    sentence_list.append((word,\"dummy\"))\n",
        "  one_sentence.append(sentence_list)\n",
        "\n",
        "  import numpy as np\n",
        "  np.seterr(divide='ignore', invalid='ignore')\n",
        "  synCorpus_words=[]\n",
        "  line=0\n",
        "\n",
        "\n",
        "  for sent in one_sentence:\n",
        "    synCorpus_words.append([])\n",
        "    for (word,tag) in sent:\n",
        "        synCorpus_words[line].append(word)\n",
        "    line+=1\n",
        "\n",
        "  # Find context bag for each sentence, the ambiguous word is ignored when comparing with the ambigous word's sense\n",
        "  length_of_wordvec=len(model_w2v[\"the\"])\n",
        "  context_bag_vector = []\n",
        "  for sent in synCorpus_words:\n",
        "      context_count=0\n",
        "      context_list=np.zeros((length_of_wordvec,))\n",
        "      #for every sentence get the context bag\n",
        "      single_sentence='_'.join(word for word in sent)\n",
        "      single_sentence=single_sentence.replace('-','_')\n",
        "      splits=single_sentence.split('_')\n",
        "      for word in splits:\n",
        "          try:\n",
        "              context_list+=model_w2v[word]\n",
        "              context_count+=1\n",
        "          except:\n",
        "              pass\n",
        "      if context_count!=0:\n",
        "          context_bag_vector.append(context_list/context_count)\n",
        "      else:\n",
        "          context_bag_vector.append(context_list)\n",
        "\n",
        "\n",
        "  # Find similarity with sense bag for each sense\n",
        "  overlapCorpus=[]\n",
        "  count=0\n",
        "  test=0\n",
        "  test1=0\n",
        "  for sent in synCorpus_words:\n",
        "      overlapCorpus.append([])\n",
        "      sentence_pos_tags=generate_pos_tags(sentence)[0][1:-1]\n",
        "      curr_word_index=-1\n",
        "      for word in sent:\n",
        "          curr_word_index+=1\n",
        "          synsets_word=wn.synsets(word)\n",
        "          if len(synsets_word) ==0:\n",
        "            overlapCorpus[count].append((word,'NOT IN WORDNET'))\n",
        "            continue\n",
        "          else:\n",
        "            if sentence_pos_tags[curr_word_index]=='NOUN':\n",
        "              if len(wn.synsets(word,pos=wn.NOUN))!=0:\n",
        "                synsets_word=wn.synsets(word,pos=wn.NOUN)\n",
        "            elif sentence_pos_tags[curr_word_index]=='ADJ':\n",
        "              if len(wn.synsets(word,pos=wn.ADJ))!=0:\n",
        "                synsets_word=wn.synsets(word,pos=wn.ADJ)\n",
        "            elif sentence_pos_tags[curr_word_index]=='ADV':\n",
        "              if len(wn.synsets(word,pos=wn.ADV))!=0:\n",
        "                synsets_word=wn.synsets(word,pos=wn.ADV)\n",
        "            elif sentence_pos_tags[curr_word_index]=='VERB':\n",
        "              if len(wn.synsets(word,pos=wn.VERB))!=0:\n",
        "                synsets_word=wn.synsets(word,pos=wn.VERB)\n",
        "            else:\n",
        "              pass\n",
        "          curr_sim=-1\n",
        "          try:\n",
        "            curr_synset=synsets_word[0]\n",
        "          except:\n",
        "            pass\n",
        "          for synset_word in synsets_word: # for each sense of a word\n",
        "              amb_list=np.zeros((length_of_wordvec,))\n",
        "              amb_count=0\n",
        "              synset_gloss=synset_word.definition() # get the word's gloss\n",
        "              all_eg=synset_word.examples()\n",
        "              synset_all_eg=\"\"\n",
        "              for i in range(len(all_eg)):\n",
        "                if all_eg[i]==' ':\n",
        "                  break\n",
        "                synset_all_eg+=\" \"+all_eg[i]\n",
        "              synset_gloss+=\" \"+synset_all_eg\n",
        "              splits=synset_gloss.split()\n",
        "              for word1 in splits:\n",
        "                  try:\n",
        "                      if(word1!=word):\n",
        "                          amb_list+=model_w2v[word1]\n",
        "                          amb_count+=1\n",
        "                  except:\n",
        "                      pass\n",
        "              if amb_count!=0: # for context bag wrt an ambiguous word\n",
        "                  amb_vec=(amb_list/amb_count)\n",
        "              else:\n",
        "                  amb_vec=amb_list\n",
        "              cos_similarity=model_w2v.cosine_similarities(context_bag_vector[count].T,[amb_vec.T])[0] # Check cosine similary\n",
        "              if curr_sim<cos_similarity: # and update the sense to the most similar sense\n",
        "                  curr_sim=cos_similarity\n",
        "                  curr_synset=synset_word\n",
        "          overlapCorpus[count].append((word,curr_synset.definition()))\n",
        "      count+=1\n",
        "\n",
        "  return overlapCorpus[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x4VKAhh1JN_",
        "outputId": "b278b7f3-69ec-416e-b7c6-97de1c4a0f24"
      },
      "source": [
        "test(\"I went to the bank to withdraw some money.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['^^', 'I', 'went', 'to', 'the', 'bank', 'to', 'withdraw', 'some', 'money.', '$$']]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'the smallest whole number or a numeral representing this number'),\n",
              " ('went', \"have a turn; make one's move in a game\"),\n",
              " ('to', 'NOT IN WORDNET'),\n",
              " ('the', 'NOT IN WORDNET'),\n",
              " ('bank',\n",
              "  'a financial institution that accepts deposits and channels the money into lending activities'),\n",
              " ('to', 'NOT IN WORDNET'),\n",
              " ('withdraw', 'pull back or move away or backward'),\n",
              " ('some', 'relatively much but unspecified in amount or extent'),\n",
              " ('money', 'the official currency issued by a government or national bank'),\n",
              " ('.', 'NOT IN WORDNET')]"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    }
  ]
}