{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Universal Viterbi Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXDQ8OlVDJAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64678ac-b625-43f1-d5aa-9ad14aed116f"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib\n",
        "import nltk\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "def generate_pos_tags(test_sentence):\n",
        "               \n",
        "\t\tfrom nltk.corpus import brown\n",
        "\n",
        "\n",
        "\n",
        "\t\t#Use universal tagset , currently not using\n",
        "\t\tsentence_tag = nltk.corpus.brown.tagged_sents(tagset=\"universal\")\n",
        "\t\tmodified_sentence_tag=[]\n",
        "\t\tfor sent in sentence_tag:\n",
        "\t\t\tsent.insert(0,('^^','^^'))         # Sentence starts with '^^'\n",
        "\t\t\tsent.append(('$$','$$'))           # Sentence ends with '$$'\n",
        "\t\t\tmodified_sentence_tag.append(sent)\n",
        "\n",
        "\n",
        "\t\ttrain_dataset = modified_sentence_tag\n",
        "\n",
        "\t\tmodified_test_sentence = '^^ '+ test_sentence\n",
        "\n",
        "\t\t#needs to be created\n",
        "\t\t# test_dataset = modified_test_sentence.split(' ')\n",
        "\t\t# print(test_dataset)\n",
        "\t\ttest_dataset=nltk.word_tokenize(modified_test_sentence)\n",
        "\t\ttest_dataset.append('$$')\n",
        "\t\t#Creation of a dictionary whose keys are tags and values contain words which have correspoding tag in the taining dataset\n",
        "\t\t#example:- 'TAG':{word1: count(word1,'TAG')} count(word1,'TAG') means how many times the word is tagged as 'TAG'\n",
        "\t\ttrain_word_tag = {}\n",
        "\t\tfor sent in train_dataset:\n",
        "\t\t\tfor (word,tag) in sent:\n",
        "\t\t\t\tword=word.lower()            # removing ambiguity from capital letters \n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\ttrain_word_tag[tag][word]+=1\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\ttrain_word_tag[tag][word]=1\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\ttrain_word_tag[tag]={word:1}\n",
        "\n",
        "\n",
        "\t\t#Calculation of emission probabilities using train_word_tag\n",
        "\t\ttrain_emission_prob={}\n",
        "\t\tfor key in train_word_tag.keys():\n",
        "\t\t\ttrain_emission_prob[key]={}\n",
        "\t\t\tcount = sum(train_word_tag[key].values())                           # count is total number of words tagged as a 'TAG'\n",
        "\t\t\tfor key2 in train_word_tag[key].keys():\n",
        "\t\t\t\ttrain_emission_prob[key][key2]=train_word_tag[key][key2]/count    \n",
        "\n",
        "\t\t#Emission probability is #times a word occured as 'TAG' / total number of 'TAG' words\n",
        "\t\t#example: number of times 'Sandeep' occured as Noun / total number of nouns\n",
        "\n",
        "\n",
        "\t\t#Estimating the bigrams of tags to be used for calculation of transition probability \n",
        "\t\t#Bigram Assumption is made, the current tag depends only on the previous tag\n",
        "\t\tbigram_tag_data = {}\n",
        "\t\tfor sent in train_dataset:\n",
        "\t\t\tbi=list(nltk.bigrams(sent))\n",
        "\t\t\tfor b1,b2 in bi:\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\tbigram_tag_data[b1[1]][b2[1]]+=1\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\tbigram_tag_data[b1[1]][b2[1]]=1\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tbigram_tag_data[b1[1]]={b2[1]:1}\n",
        "\t\t#bigram_tag_data is storing the values for every tag.\n",
        "\t\t#Every key is a tag and value is tag followed for that key and corresponding counts.\n",
        "\t\t#example: how many times an adj is followed by a noun {Noun:{Adj:3}}, here its 3 times.\n",
        "\n",
        "\t\t#Calculation of the probabilities of tag bigrams for transition probability\n",
        "\t\t#We already made a bigram assumption   \n",
        "\t\tbigram_tag_prob={}\n",
        "\t\tfor key in bigram_tag_data.keys():\n",
        "\t\t\tbigram_tag_prob[key]={}\n",
        "\t\t\tcount=sum(bigram_tag_data[key].values())              # count is total number of times a 'TAG' has occured\n",
        "\t\t\tfor key2 in bigram_tag_data[key].keys():\n",
        "\t\t\t\tbigram_tag_prob[key][key2]=bigram_tag_data[key][key2]/count\n",
        "\t\t#Tranmission probability is #times a TAG2 is preceded by TAG1 / total number of times TAG1 exists in dataset\n",
        "\t\t#example: number of times a noun occured before adjective / total number of times a noun occurred\n",
        "\n",
        "\n",
        "\t\t#Calculation the possible tags for each word in the entire dataset\n",
        "\t\t#Note: Here we have used the whole data(Train dataset + Test dataset)\n",
        "\t\t#Reason: Words present in Test data is not subset pf Train data.\n",
        "\t\t#The above thing can be neglected if not necessay, but it improves our accuracy of the model \n",
        "\t\ttags_of_tokens = {}\n",
        "\t\tcount=0\n",
        "\t\tfor sent in train_dataset:\n",
        "\t\t\tfor (word,tag) in sent:\n",
        "\t\t\t\tword=word.lower()\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\tif tag not in tags_of_tokens[word]:\n",
        "\t\t\t\t\t\ttags_of_tokens[word].append(tag)\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tlist_of_tags = []\n",
        "\t\t\t\t\tlist_of_tags.append(tag)\n",
        "\t\t\t\t\ttags_of_tokens[word] = list_of_tags\n",
        "\t\t#Each word and its corresponding tags in the train dataset\n",
        "\n",
        "\n",
        "\n",
        "\t\t#Seperating the test data into test words\n",
        "\t\ttest_words= [test_dataset]\n",
        "\n",
        "\t\t\n",
        "\t\tpredicted_tags = []                #Final list for prediction\n",
        "\t\tfor i in range(len(test_words)):   # for each tokenized sentence in the test data (test_words is a list of lists)\n",
        "\t\t\tsent = test_words[i]\n",
        "\t\t\t#storing_values is a dictionary which stores the required values\n",
        "\t\t\t#ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
        "\t\t\tstoring_values = {}              \n",
        "\t\t\tfor q in range(len(sent)):\n",
        "\t\t\t\tstep = sent[q]\n",
        "\t\t\t\t#for the starting word of the sentence\n",
        "\t\t\t\tif q == 1:                \n",
        "\t\t\t\t\tstoring_values[q] = {}\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\ttags = tags_of_tokens[step]\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t# print(step,test_tags_of_tokens[step])\n",
        "\t\t\t\t\t\ttags=['NOUN'] #tags_of_unseen_tokens\n",
        "\t\t\t\t\tfor t in tags:\n",
        "\t\t\t\t\t\t#this is applied since we do not know whether the word in the test data is present in train data or not\n",
        "\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\tstoring_values[q][t] = ['^^',bigram_tag_prob['^^'][t]*train_emission_prob[t][step]]\n",
        "\t\t\t\t\t\t\t#if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
        "\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\tstoring_values[q][t] = ['^^',0.0001]\n",
        "\t\t  \n",
        "\t\t\t\t  #if the word is not at the start of the sentence\n",
        "\t\t\t\tif q>1:\n",
        "\t\t\t\t\tstoring_values[q] = {}\n",
        "\t\t\t\t\tprevious_states = list(storing_values[q-1].keys())   # loading the previous states\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\tcurrent_states  = tags_of_tokens[step]               # loading the current states\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\tcurrent_states = ['NOUN']#tags_of_unseen_tokens\n",
        "\t\t\t\t\t\t#calculation of the best previous state for each current state and then storing\n",
        "\t\t\t\t\t\t#it in storing_values\n",
        "\t\t\t\t\tfor t in current_states:                             \n",
        "\t\t\t\t\t\ttemp = []\n",
        "\t\t\t\t\t\tfor pt in previous_states:                         \n",
        "\t\t\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t\t  temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step]) # If seen word\n",
        "\t\t\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\t\t  temp.append(storing_values[q-1][pt][1]*0.0001)\n",
        "\t\t\t\t\t\tmax_temp_index = temp.index(max(temp))\n",
        "\t\t\t\t\t\tbest_pt = previous_states[max_temp_index]\n",
        "\t\t\t\t\t\tstoring_values[q][t]=[best_pt,max(temp)] #Store the best previous tag for each best candidate per tag and the meximum probability\n",
        "\t\t \n",
        "\t\t\t#Backtracing to extract the best possible tags for the sentence\n",
        "\t\t\t# for each word looking the current word and the word and tag next to it in the sentence backtrack\n",
        "\t\t\t# to get the tag of current word\n",
        "\t\t\tpred_tags = [] #predicted tags by viterbi using backtracking\n",
        "\t\t\ttotal_steps_num = storing_values.keys()\n",
        "\t\t\tlast_step_num = max(total_steps_num)     # Begin from the last word which will end the delimiter\n",
        "\t\t\tfor bs in range(len(total_steps_num)):    \n",
        "\t\t\t\tstep_num = last_step_num - bs          \n",
        "\t\t\t\tif step_num == last_step_num:\n",
        "\t\t\t\t\tpred_tags.append('$$')\n",
        "\t\t\t\t\tpred_tags.append(storing_values[step_num]['$$'][0]) \n",
        "\t\t\t\tif step_num<last_step_num and step_num>0:\n",
        "\t\t\t\t\tpred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0]) #Looking into storing value fetch the best previous tag for the current word\n",
        "\t\t\tpredicted_tags.append(list(reversed(pred_tags)))\n",
        "\n",
        "\n",
        "\t\tprint(test_words)\n",
        "\n",
        "\t\tprint(predicted_tags)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Faid_YDIDQq5",
        "outputId": "539f63dc-b590-40d5-d67c-9961081fcc5c"
      },
      "source": [
        "\n",
        "test_string = \"Pushpak sir teaches us NLP.\"\n",
        "\n",
        "generate_pos_tags(test_string)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['^^', 'Pushpak', 'sir', 'teaches', 'us', 'NLP', '.', '$$']]\n",
            "[['^^', 'NOUN', 'NOUN', 'VERB', 'PRON', 'NOUN', '.', '$$']]\n"
          ]
        }
      ]
    }
  ]
}