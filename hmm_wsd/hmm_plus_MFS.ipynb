{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5a7704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from NLTK please wait...\n",
      "NLTK files downloaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import statistics\n",
    "import pandas as pd\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Getting the brown corpus\n",
    "print(\"Downloading files from NLTK please wait...\")\n",
    "nltk.download('all', quiet=True)\n",
    "print(\"NLTK files downloaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda1ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing the semcor corpus...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing the semcor corpus...\")\n",
    "synCorpus=[]\n",
    "current_index=0\n",
    "for taggedSents in semcor.tagged_sents(tag='sem'): # for each sentence fetch the (phrase,synsetTree)\n",
    "    synCorpus.append([])                           # create a list designating current sentence which will contain (phrase,synsetID)\n",
    "    for phrase in taggedSents:\n",
    "        if type(phrase)==list:                     # The phrase with no tag is a list in semcor [Note: CASE 1]\n",
    "            pass\n",
    "            #if its a list then its a non tagged word hence ignore it\n",
    "        else: # if synset is a Tree [Note Case 2 and 3]\n",
    "            # Here when theres a synset not in wordnet, theres no synset id[Case 3], to handle this\n",
    "            # except block is used\n",
    "            # The try except will get the synset ID and in case no synsetID -2 is given to such a case\n",
    "            try: \n",
    "                # handle Case 2\n",
    "                synTag_for_corpus=phrase.label().synset().offset()\n",
    "            except:\n",
    "                # handle Case 3\n",
    "                pass # -2 indicates no synset tags for the specific POS but available for some POS\n",
    "            if isinstance(phrase.label(),str) ==True:\n",
    "                synText_for_corpus=phrase.label().split(\".\")[0]\n",
    "            else:\n",
    "                synText_for_corpus=phrase.label().name()\n",
    "            synCorpus[current_index].append((synText_for_corpus,synTag_for_corpus)) # For case 2 and 3 form a tuple\n",
    "    current_index+=1 # Keeps tract of the sentence index\n",
    "no_sents=current_index #Saved total number of sentences\n",
    "synCorpus.append([])\n",
    "synCorpus[current_index].append((\" \",-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0993bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfs_dict = {}\n",
    "only_words=[]\n",
    "original_ids=[]\n",
    "for sent in synCorpus:\n",
    "    sentence=[]\n",
    "    sent_id=[]\n",
    "    for (word,tag) in sent:\n",
    "        word=word.lower()\n",
    "        try:\n",
    "            try:\n",
    "                mfs_dict[word][tag]+=1\n",
    "            except:\n",
    "                mfs_dict[word][tag]=1\n",
    "        except:\n",
    "            mfs_dict[word]={tag:1}\n",
    "        sentence.append(word)\n",
    "        sent_id.append(tag)\n",
    "    only_words.append(sentence)\n",
    "    original_ids.append(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d199b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e52b8694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semcor preprocessing finished\n",
      "Running Viterbi over all 5 cross validation sets...\n",
      "Set  1 ✓\n",
      "Set  2 ✓\n",
      "Set  3 ✓\n",
      "Set  4 ✓\n",
      "Set  5 ✓\n",
      "===================\n",
      "OVERALL ESTIMATIONS\n",
      "===================\n",
      "Overall Precision: 0.569788 ± 0.001143\n",
      "Overall Recall: 0.569788 ± 0.001143\n",
      "Overall F1 Score: 0.569788 ± 0.001143\n",
      "Overall F0.5 Score: 0.569788 ± 0.001143\n",
      "Overall F2 Score: 0.569788 ± 0.001143\n"
     ]
    }
   ],
   "source": [
    "# The reason for storing as tuples is because the brown corpus stored it in a same way and this way we have to make minimal changed to HMM-Viterbi POS tag code if any :)\n",
    "print(\"Semcor preprocessing finished\")\n",
    "\n",
    "# Sentences fetched and prefixed and suffixed by delimiters. If originally sentence contains a \n",
    "# delimiter then set it as ('<DEL>','X')\n",
    "sentence_tag = synCorpus\n",
    "modified_sentence_tag=[]\n",
    "for sent in sentence_tag:\n",
    "  for word,tag in sent:\n",
    "    if word=='^^' or word=='$$':\n",
    "      word='<DEL>'\n",
    "      tag='X'\n",
    "  sent.insert(0,('^^','^^'))         # Sentence starts with '^^'\n",
    "  sent.append(('$$','$$'))           # Sentence ends with '$$'\n",
    "  modified_sentence_tag.append(sent)\n",
    "\n",
    "# Shuffle the whole corpus uniformly\n",
    "random.shuffle(modified_sentence_tag)\n",
    "\n",
    "# Divide corpus into 5 equal parts\n",
    "sentences_set1=modified_sentence_tag[:math.floor(len(modified_sentence_tag)*1/5)]\n",
    "sentences_set2=modified_sentence_tag[math.floor(len(modified_sentence_tag)*1/5):math.floor(len(modified_sentence_tag)*2/5)]\n",
    "sentences_set3=modified_sentence_tag[math.floor(len(modified_sentence_tag)*2/5):math.floor(len(modified_sentence_tag)*3/5)]\n",
    "sentences_set4=modified_sentence_tag[math.floor(len(modified_sentence_tag)*3/5):math.floor(len(modified_sentence_tag)*4/5)]\n",
    "sentences_set5=modified_sentence_tag[math.floor(len(modified_sentence_tag)*4/5):]\n",
    "\n",
    "train_sentences=[[],[],[],[],[]]\n",
    "test_sentences=[[],[],[],[],[]]\n",
    "\n",
    "# For 5 Fold Cross Validation Train and test set\n",
    "# Set1 as test set\n",
    "train_sentences[0]=sentences_set2+sentences_set3+sentences_set4+sentences_set5\n",
    "test_sentences[0]=sentences_set1\n",
    "\n",
    "# Set2 as test set\n",
    "train_sentences[1]=sentences_set1+sentences_set3+sentences_set4+sentences_set5\n",
    "test_sentences[1]=sentences_set2\n",
    "\n",
    "# Set3 as test set\n",
    "train_sentences[2]=sentences_set1+sentences_set2+sentences_set4+sentences_set5\n",
    "test_sentences[2]=sentences_set3\n",
    "\n",
    "# Set4 as test set\n",
    "train_sentences[3]=sentences_set1+sentences_set2+sentences_set3+sentences_set5\n",
    "test_sentences[3]=sentences_set4\n",
    "\n",
    "# Set5 as test set\n",
    "train_sentences[4]=sentences_set1+sentences_set2+sentences_set3+sentences_set4\n",
    "test_sentences[4]=sentences_set5\n",
    "\n",
    "precision_sets=[0]*5\n",
    "recall_sets=[0]*5\n",
    "F1_score_sets=[0]*5\n",
    "F05_score_sets=[0]*5\n",
    "F2_score_sets=[0]*5\n",
    "pos_estimation_sets=[pd.DataFrame]*5\n",
    "print(\"Running Viterbi over all 5 cross validation sets...\") #Note this is just for showing progress, viterbi is executed further in the loop\n",
    "for setno in range(5):\n",
    "  # For each set calculate the transmission and emission probabilities on training set\n",
    "  # And perform viterbi on test set. Later find per POS and overall estimation\n",
    "  train_dataset = train_sentences[setno]\n",
    "  test_dataset = test_sentences[setno]\n",
    "\n",
    "  ## EMISSION PROBABILITY TABLE\n",
    "  # Creation of a dictionary whose keys are tags and values contain words which have corresponding tag in the taining dataset\n",
    "  # example:- 'TAG':{word1: count(word1,'TAG')} count(word1,'TAG') means how many times the word is tagged as 'TAG'\n",
    "  train_word_tag = {}\n",
    "  for sent in train_dataset:\n",
    "    for (word,tag) in sent:\n",
    "      word=word.lower()            # removing ambiguity from capital letters \n",
    "      try:\n",
    "        try:\n",
    "          train_word_tag[tag][word]+=1\n",
    "        except:\n",
    "          train_word_tag[tag][word]=1\n",
    "      except:\n",
    "          train_word_tag[tag]={word:1}\n",
    "\n",
    "  #Calculation of emission probabilities using train_word_tag\n",
    "  train_emission_prob={}\n",
    "  for key in train_word_tag.keys():\n",
    "    train_emission_prob[key]={}\n",
    "    count = sum(train_word_tag[key].values())                           # count is total number of words tagged as a 'TAG'\n",
    "    for key2 in train_word_tag[key].keys():\n",
    "      train_emission_prob[key][key2]=train_word_tag[key][key2]/count    \n",
    "  #Emission probability is #times a word occured as 'TAG' / total number of 'TAG' words\n",
    "\n",
    "  ## TRANSITION PROBABILITY TABLE\n",
    "  #Estimating the bigrams of tags to be used for calculation of transition probability \n",
    "  #Bigram Assumption is made, the current tag depends only on the previous tag\n",
    "  bigram_tag_data = {}\n",
    "  for sent in train_dataset:\n",
    "    bi=list(nltk.bigrams(sent))\n",
    "    for b1,b2 in bi:\n",
    "      try:\n",
    "        try:\n",
    "          bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "        except:\n",
    "          bigram_tag_data[b1[1]][b2[1]]=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]]={b2[1]:1}\n",
    "\n",
    "  #bigram_tag_data is storing the values for every tag.\n",
    "  #Every key is a tag and value is tag followed for that key and corresponding counts.\n",
    "\n",
    "  #Calculation of the probabilities of tag bigrams for transition probability\n",
    "  #We already made a bigram assumption  \n",
    "  #Also note that since we are also considering $, the $ row of transition probability matrix give us the initial probabilities as well\n",
    "  bigram_tag_prob={}\n",
    "  for key in bigram_tag_data.keys():\n",
    "    bigram_tag_prob[key]={}\n",
    "    count=sum(bigram_tag_data[key].values())              # count is total number of times a 'TAG' has occured\n",
    "    for key2 in bigram_tag_data[key].keys():\n",
    "      bigram_tag_prob[key][key2]=bigram_tag_data[key][key2]/count\n",
    "  #Tranmission probability is #times a TAG2 is preceded by TAG1 / total number of times TAG1 exists in dataset\n",
    "  \n",
    "  #Calculation the possible tags for each word in the train dataset\n",
    "  tags_of_tokens = {}\n",
    "  count=0\n",
    "  for sent in train_dataset:\n",
    "    for (word,tag) in sent:\n",
    "      word=word.lower()\n",
    "      try:\n",
    "        if tag not in tags_of_tokens[word]:\n",
    "          tags_of_tokens[word].append(tag)\n",
    "      except:\n",
    "        list_of_tags = []\n",
    "        list_of_tags.append(tag)\n",
    "        tags_of_tokens[word] = list_of_tags\n",
    "  #Each word and its corresponding tags in the train dataset\n",
    "\n",
    "  # Getting words and their corresponding tags from the test set\n",
    "  # Seperating the test data into test words and test tags\n",
    "  test_words=[]\n",
    "  test_tags=[]\n",
    "  for sent in test_dataset:\n",
    "    temp_word=[]\n",
    "    temp_tag=[]\n",
    "    for (word,tag) in sent:\n",
    "      temp_word.append(word.lower()) # words of a sentence in test dataset\n",
    "      temp_tag.append(tag) # tags of a sentence in test dataset\n",
    "    test_words.append(temp_word) # list with words of a sentence(tokenized sentence) appended to a list of list\n",
    "    test_tags.append(temp_tag) # list with tags of a sentence(tokenized sentence) appended to a list of list\n",
    "\n",
    "  #VITERBI ALGORITHM IMPLEMENTATION\n",
    "# For each word in a sentence\n",
    "# – The probability of best candidates for each tag at previous level is\n",
    "#   multiplied with the emission probability and the transition\n",
    "#   probability of possible tags based on current word\n",
    "# – The best candidate for each tag at the current level is chosen and\n",
    "#   the previous tag is kept a track of for backtracking\n",
    "# – The result of forward propagation at each level looks like the\n",
    "#   following\n",
    "#   LEVEL_K:{TAG1:{best_candidate_among_Tag1,probability_of the\n",
    "#   best candidate} , {TAG2:{previous_tag_of_best_candidate_\n",
    "#   among_Tag2, probability_of the best candidate}, ...}\n",
    "# – When backtracking start from the end to start choosing the\n",
    "#   predicted tag based on LEVEL information and the predicted tag\n",
    "#   that follows the word.\n",
    "  predicted_tags = []                #Final list for prediction\n",
    "  for i in range(len(test_words)):   # for each tokenized sentence in the test data (test_words is a list of lists)\n",
    "    sent = test_words[i]\n",
    "    #storing_values is a dictionary which stores the required values\n",
    "    #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
    "    storing_values = {}              \n",
    "    for q in range(len(sent)):\n",
    "      step = sent[q]\n",
    "      #for the starting word of the sentence\n",
    "      if q == 1:                \n",
    "        storing_values[q] = {}\n",
    "        try:\n",
    "          tags = tags_of_tokens[step]\n",
    "        except:\n",
    "          # print(step,test_tags_of_tokens[step])\n",
    "          try:\n",
    "                tags=[max(zip(mfs_dict[step].values(),mfs_dict[step].keys()))[1]] #Check for it in WordNet\n",
    "                \n",
    "          except:\n",
    "                tags=[-1] #If not even in wordnet give a dummy id\n",
    "#                 print(step,\"q=1\")\n",
    "        for t in tags:\n",
    "          #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "          try:\n",
    "            storing_values[q][t] = ['^^',bigram_tag_prob['^^'][t]*train_emission_prob[t][step]]\n",
    "          #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "          except:\n",
    "            storing_values[q][t] = ['^^',0.0001]\n",
    "      \n",
    "      #if the word is not at the start of the sentence\n",
    "      if q>1:\n",
    "        storing_values[q] = {}\n",
    "        previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "        try:\n",
    "          current_states  = tags_of_tokens[step]               # loading the current states\n",
    "        except:\n",
    "          try:\n",
    "                 current_states=[max(zip(mfs_dict[step].values(),mfs_dict[step].keys()))[1]] #Check for it in WordNet\n",
    "          except:\n",
    "                 current_states=[-1] #If not even in wordnet give a dummy id\n",
    "#                  print(step,\"q>1\")\n",
    "        #calculation of the best previous state for each current state and then storing\n",
    "        #it in storing_values\n",
    "        for t in current_states:                             \n",
    "          temp = []\n",
    "          for pt in previous_states:                         \n",
    "            try:\n",
    "              temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step]) # If seen word\n",
    "            except:\n",
    "              temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "          max_temp_index = temp.index(max(temp))\n",
    "          best_pt = previous_states[max_temp_index]\n",
    "          storing_values[q][t]=[best_pt,max(temp)] #Store the best previous tag for each best candidate per tag and the meximum probability\n",
    " \n",
    "    #Backtracing to extract the best possible tags for the sentence\n",
    "    # for each word looking the current word and the word and tag next to it in the sentence backtrack\n",
    "    # to get the tag of current word\n",
    "    pred_tags = [] #predicted tags by viterbi using backtracking\n",
    "    total_steps_num = storing_values.keys()\n",
    "    last_step_num = max(total_steps_num)     # Begin from the last word which will end the delimiter\n",
    "    for bs in range(len(total_steps_num)):    \n",
    "      step_num = last_step_num - bs          \n",
    "      if step_num == last_step_num:\n",
    "        pred_tags.append('$$')\n",
    "        pred_tags.append(storing_values[step_num]['$$'][0]) \n",
    "      if step_num<last_step_num and step_num>0:\n",
    "        pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0]) #Looking into storing value fetch the best previous tag for the current word\n",
    "    predicted_tags.append(list(reversed(pred_tags)))\n",
    "\n",
    "\n",
    "#Now that the tags are predicted, get the actual and predicted tags so that analysis can be done\n",
    "  tag_seq_act=[]\n",
    "  tag_seq_pred=[]\n",
    "  uniq_tag=set()\n",
    "  uniq_tag_dict={}\n",
    "  for li in test_tags:\n",
    "      for tag in li:\n",
    "          if(tag!=\"^^\" and tag!=\"$$\"): #Exclude delimiters\n",
    "            tag_seq_act.append(tag)\n",
    "\n",
    "  for li in predicted_tags:\n",
    "      for tag in li:\n",
    "          if(tag!=\"^^\" and tag!=\"$$\"): #Exclude delimiters\n",
    "            tag_seq_pred.append(tag)\n",
    "      \n",
    "  for tag in tag_seq_act:\n",
    "      uniq_tag.add(tag)\n",
    "   \n",
    "##### Added this fix because there was key error since uniq _tag tooks only test tags into consideration      \n",
    "  for tag in tag_seq_pred:\n",
    "      if tag not in uniq_tag:\n",
    "      \tuniq_tag.add(tag)\n",
    "##### But I dont think that this fix works >.<\n",
    "  uniq_tag=list(uniq_tag)\n",
    "    \n",
    "  for i in range(len(uniq_tag)):\n",
    "      uniq_tag_dict[uniq_tag[i]]=i\n",
    "              \n",
    "  for i,tag in enumerate(tag_seq_act):\n",
    "      tag_seq_act[i]=uniq_tag_dict[tag]\n",
    "\n",
    "  for i,tag in enumerate(tag_seq_pred):\n",
    "      tag_seq_pred[i]=uniq_tag_dict[tag]\n",
    "\n",
    "# Calculate the precision, Recall and F-Scores by comparing the actual and predicted tags\n",
    "  matched_tags=0\n",
    "  for i in range(len(tag_seq_act)):\n",
    "      if tag_seq_act[i]==tag_seq_pred[i]:\n",
    "        matched_tags+=1\n",
    "  # Estimations for the current set\n",
    "  precision=matched_tags/(len(tag_seq_pred))\n",
    "  recall=matched_tags/(len(tag_seq_act))\n",
    "  F1_score=(2*precision*recall)/(precision+recall)\n",
    "  F05_score=(1.25*precision*recall)/(0.25*precision+recall)\n",
    "  F2_score=(5*precision*recall)/(4*precision+recall)\n",
    "\n",
    "\n",
    "  #Store every set's estimations\n",
    "  precision_sets[setno]=precision\n",
    "  recall_sets[setno]=recall\n",
    "  F1_score_sets[setno]=F1_score\n",
    "  F05_score_sets[setno]=F05_score\n",
    "  F2_score_sets[setno]=F2_score\n",
    "  #pos_estimation_sets[setno]=pos_estimation\n",
    "  print(\"Set \",setno+1,\"✓\")\n",
    "\n",
    "# After getting every set's estimations combine them\n",
    "# For k fold cross validation\n",
    "# mean(each of the k set estimations) ± standard_error(each of the k set estimations)\n",
    "# standard_error= standard_deviation/square_root(k)\n",
    "# example Overall precision=(Σ precision_set i)/5 ± squareroot([Σ(precision_set_i-precision_mean)²]/5) here 5 for 5 fold cross validation\n",
    "print(\"===================\\nOVERALL ESTIMATIONS\\n===================\")\n",
    "print(\"Overall Precision:\", \"{:.6f}\".format(statistics.mean(precision_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(precision_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall Recall:\" ,\"{:.6f}\".format(statistics.mean(recall_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(recall_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall F1 Score:\", \"{:.6f}\".format(statistics.mean(F1_score_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(F1_score_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall F0.5 Score:\", \"{:.6f}\".format(statistics.mean(F1_score_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(F1_score_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall F2 Score:\", \"{:.6f}\".format(statistics.mean(F1_score_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(F1_score_sets)/math.sqrt(setno+1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baffdd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47502"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_seq_act)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
