{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7589084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from NLTK please wait...\n",
      "NLTK files downloaded!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib\n",
    "import nltk\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "# display_conf_matrix() takes the confusion matrix and produces a visual of it\n",
    "def display_conf_matrix(conf, labels):\n",
    "  # Set font values for the confusion matrix image\n",
    "    font = {'family' : 'DejaVu Sans',\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 10}\n",
    "    plt.rc('font', **font)\n",
    "    cm_obj=ConfusionMatrixDisplay(conf, display_labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(30,30))\n",
    "    ax.set_xticklabels(labels,fontsize=9)\n",
    "    ax.set_yticklabels(labels,fontsize=9)\n",
    "    # values_format='' is used to avoid showing in precision format eg 4200 shoudn't be shown as 4.2e3 (just to make the confusion matrix look better) \n",
    "    cm_obj.plot(ax=ax,values_format='')\n",
    "    # set the axis labels and title\n",
    "    cm_obj.ax_.set(\n",
    "            title=\"Confusion Matrix\",\n",
    "            xlabel=\"Predicted\",\n",
    "            ylabel=\"Actual\"\n",
    "    )\n",
    "    ax.xaxis.label.set_size(30)\n",
    "    ax.yaxis.label.set_size(30)\n",
    "    ax.title.set_size(30)\n",
    "\n",
    "\n",
    "# per_POS_evaluation() calculates the per POS performance\n",
    "# For each pair for tags(one from row and other column) in confusion matrix\n",
    "# calculate the precision, recall and F1 Score\n",
    "# All these scores are stored in a dataframe and the dataframe is returned\n",
    "def per_POS_evaluation(conf_matrix,uniq_tag):\n",
    "    li=[]\n",
    "    for i in range(len(conf_matrix)):\n",
    "        rt,ct=0,0\n",
    "        # rt= no. of actual tags for tag i\n",
    "        # rt= no. of predicted(obtained) tags for tag i\n",
    "        for j in range(len(conf_matrix)):\n",
    "            rt+=conf_matrix[i][j]\n",
    "            ct+=conf_matrix[j][i]\n",
    "        A=conf_matrix[i][i]\n",
    "        # Calculate scores for each POS tag \n",
    "        prec=A/ct\n",
    "        rec=A/rt\n",
    "        F1=(2*prec*rec)/(prec+rec)\n",
    "        li.append([prec,rec,F1])\n",
    "    di={}\n",
    "    i=0\n",
    "    for l in li:\n",
    "        di[uniq_tag[i]]=l\n",
    "        i+=1\n",
    "    table=pd.DataFrame.from_dict(di, orient='index')\n",
    "    table.columns=['Precision', 'Recall', 'F1_Score']\n",
    "    return table\n",
    "\n",
    "# Getting the brown corpus\n",
    "print(\"Downloading files from NLTK please wait...\")\n",
    "nltk.download('all', quiet=True)\n",
    "print(\"NLTK files downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37363cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3916135765.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_12707/3916135765.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    else: # if synset is a Tree [Note Case 2 and 3]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Task: Tag words with synset ID\n",
    "# Three types of tags in semcor\n",
    "# 1) No tag\n",
    "# 2) Tagged with a lemma of synset: A lemma is a data structure which contains synset along with its synset_id, hypernymy, hyponymy,... \n",
    "# 3) Tagged with lemma but the synset ID doesn't exist in wordnet\n",
    "#    One such example is the 7th sentence in the semcor corpus\n",
    "#    The grand jury commented on a number of other topics, among them the Atlanta and Fulton County purchasing departments which it said are \n",
    "#    well operated and follow generally accepted practices which inure to the best interest of both governments.\n",
    "#                                       ^______^  \"accepted\" is tagged as \"accepted.s.00\" lemma and this doesn't have a synsetID in wordnet\n",
    "#    All the three cases are handled by the code\n",
    "\n",
    "# Each sentence has a pair of phrase and (representation of)synset\n",
    "# Each Phrase has one or more words(e.g. \"primary election\" in first sentence of corpora)\n",
    "# Each synset is respresented as a Tree e.g. for \"primary election\"\n",
    "# Lemma('primary.n.01.primary_election')\n",
    "#                 /\\\n",
    "#                /  \\\n",
    "#               /    \\\n",
    "#         primary   election\n",
    "# This example will be used later to explain code\n",
    "print(\"Preprocessing the semcor corpus...\")\n",
    "synCorpus=[]\n",
    "current_index=0\n",
    "for taggedSents in semcor.tagged_sents(tag='sem'): # for each sentence fetch the (phrase,synsetTree)\n",
    "    synCorpus.append([])                           # create a list designating current sentence which will contain (phrase,synsetID)\n",
    "    for phrase in taggedSents:\n",
    "        if type(phrase)==list:                     # The phrase with no tag is a list in semcor [Note: CASE 1]\n",
    "            #if its a list fetch the word and set -1 as synset ID which indicates no synset TAG\n",
    "           # synCorpus[current_index].append((phrase[0],-1))  # a tuple is formed\n",
    "        else: # if synset is a Tree [Note Case 2 and 3]\n",
    "            # Here when theres a synset not in wordnet, theres no synset id[Case 3], to handle this\n",
    "            # except block is used\n",
    "            # The try except will get the synset ID and in case no synsetID -2 is given to such a case\n",
    "            try: \n",
    "                # handle Case 2\n",
    "                synTag_for_corpus=phrase.label().synset().offset()\n",
    "            except:\n",
    "                # handle Case 3\n",
    "                synTag_for_corpus=-2 # -2 indicates no synset tags for the specific POS but available for some POS\n",
    "            for words in phrase:\n",
    "                # Here words will be \"primary\" and \"election\" in form of Tree[Referring to the example at the beginning]\n",
    "                str=\"\"\n",
    "                for word in words:\n",
    "                    str+=\" \"+word\n",
    "                # e.g. str= \"primary election \"\n",
    "                str.strip() # e.g. str= \"primary election\"\n",
    "                synText_for_corpus=str\n",
    "            synCorpus[current_index].append((synText_for_corpus,synTag_for_corpus)) # For case 2 and 3 form a tuple\n",
    "    current_index+=1 # Keeps tract of the sentence index\n",
    "no_sents=current_index #Saved total number of sentences\n",
    "# The reason for storing as tuples is because the brown corpus stored it in a same way and this way we have to make minimal changed to HMM-Viterbi POS tag code if any :)\n",
    "print(\"Semcor preprocessing finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1d8e845",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synCorpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12707/487921856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sentences fetched and prefixed and suffixed by delimiters. If originally sentence contains a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# delimiter then set it as ('<DEL>','X')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentence_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodified_sentence_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_tag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synCorpus' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sentences fetched and prefixed and suffixed by delimiters. If originally sentence contains a \n",
    "# delimiter then set it as ('<DEL>','X')\n",
    "sentence_tag = synCorpus\n",
    "modified_sentence_tag=[]\n",
    "for sent in sentence_tag:\n",
    "  for word,tag in sent:\n",
    "    if word=='^^' or word=='$$':\n",
    "      word='<DEL>'\n",
    "      tag='X'\n",
    "  sent.insert(0,('^^','^^'))         # Sentence starts with '^^'\n",
    "  sent.append(('$$','$$'))           # Sentence ends with '$$'\n",
    "  modified_sentence_tag.append(sent)\n",
    "\n",
    "# Shuffle the whole corpus uniformly\n",
    "random.shuffle(modified_sentence_tag)\n",
    "\n",
    "# Divide corpus into 5 equal parts\n",
    "sentences_set1=modified_sentence_tag[:math.floor(len(modified_sentence_tag)*1/5)]\n",
    "sentences_set2=modified_sentence_tag[math.floor(len(modified_sentence_tag)*1/5):math.floor(len(modified_sentence_tag)*2/5)]\n",
    "sentences_set3=modified_sentence_tag[math.floor(len(modified_sentence_tag)*2/5):math.floor(len(modified_sentence_tag)*3/5)]\n",
    "sentences_set4=modified_sentence_tag[math.floor(len(modified_sentence_tag)*3/5):math.floor(len(modified_sentence_tag)*4/5)]\n",
    "sentences_set5=modified_sentence_tag[math.floor(len(modified_sentence_tag)*4/5):]\n",
    "\n",
    "train_sentences=[[],[],[],[],[]]\n",
    "test_sentences=[[],[],[],[],[]]\n",
    "\n",
    "# For 5 Fold Cross Validation Train and test set\n",
    "# Set1 as test set\n",
    "train_sentences[0]=sentences_set2+sentences_set3+sentences_set4+sentences_set5\n",
    "test_sentences[0]=sentences_set1\n",
    "\n",
    "# Set2 as test set\n",
    "train_sentences[1]=sentences_set1+sentences_set3+sentences_set4+sentences_set5\n",
    "test_sentences[1]=sentences_set2\n",
    "\n",
    "# Set3 as test set\n",
    "train_sentences[2]=sentences_set1+sentences_set2+sentences_set4+sentences_set5\n",
    "test_sentences[2]=sentences_set3\n",
    "\n",
    "# Set4 as test set\n",
    "train_sentences[3]=sentences_set1+sentences_set2+sentences_set3+sentences_set5\n",
    "test_sentences[3]=sentences_set4\n",
    "\n",
    "# Set5 as test set\n",
    "train_sentences[4]=sentences_set1+sentences_set2+sentences_set3+sentences_set4\n",
    "test_sentences[4]=sentences_set5\n",
    "\n",
    "precision_sets=[0]*5\n",
    "recall_sets=[0]*5\n",
    "F1_score_sets=[0]*5\n",
    "F05_score_sets=[0]*5\n",
    "F2_score_sets=[0]*5\n",
    "pos_estimation_sets=[pd.DataFrame]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "680e3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Viterbi over all 5 cross validation sets...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12707/2356718226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# For each set calculate the transmission and emission probabilities on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# And perform viterbi on test set. Later find per POS and overall estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msetno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msetno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## EMISSION PROBABILITY TABLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Running Viterbi over all 5 cross validation sets...\") #Note this is just for showing progress, viterbi is executed further in the loop\n",
    "# for setno in range(5):\n",
    "setno=0\n",
    "# For each set calculate the transmission and emission probabilities on training set\n",
    "# And perform viterbi on test set. Later find per POS and overall estimation\n",
    "train_dataset = train_sentences[setno]\n",
    "test_dataset = test_sentences[setno]\n",
    "## EMISSION PROBABILITY TABLE\n",
    "# Creation of a dictionary whose keys are tags and values contain words which have corresponding tag in the taining dataset\n",
    "# example:- 'TAG':{word1: count(word1,'TAG')} count(word1,'TAG') means how many times the word is tagged as 'TAG'\n",
    "train_word_tag = {}\n",
    "for sent in train_dataset:\n",
    "  for (word,tag) in sent:\n",
    "    word=word.lower()            # removing ambiguity from capital letters \n",
    "    try:\n",
    "      try:\n",
    "        train_word_tag[tag][word]+=1\n",
    "      except:\n",
    "        train_word_tag[tag][word]=1\n",
    "    except:\n",
    "        train_word_tag[tag]={word:1}\n",
    " #Calculation of emission probabilities using train_word_tag\n",
    "train_emission_prob={}\n",
    "for key in train_word_tag.keys():\n",
    "  train_emission_prob[key]={}\n",
    "  count = sum(train_word_tag[key].values())                           # count is total number of words tagged as a 'TAG'\n",
    "  for key2 in train_word_tag[key].keys():\n",
    "    train_emission_prob[key][key2]=train_word_tag[key][key2]/count    \n",
    "#Emission probability is #times a word occured as 'TAG' / total number of 'TAG' words\n",
    "#example: number of times 'Sandeep' occured as Noun / total number of nouns\n",
    "\n",
    "## TRANSITION PROBABILITY TABLE\n",
    "#Estimating the bigrams of tags to be used for calculation of transition probability \n",
    "#Bigram Assumption is made, the current tag depends only on the previous tag\n",
    "bigram_tag_data = {}\n",
    "for sent in train_dataset:\n",
    "  bi=list(nltk.bigrams(sent))\n",
    "  for b1,b2 in bi:\n",
    "    try:\n",
    "      try:\n",
    "        bigram_tag_data[b1[1]][b2[1]]+=1\n",
    "      except:\n",
    "        bigram_tag_data[b1[1]][b2[1]]=1\n",
    "    except:\n",
    "      bigram_tag_data[b1[1]]={b2[1]:1}\n",
    " #bigram_tag_data is storing the values for every tag.\n",
    " #Every key is a tag and value is tag followed for that key and corresponding counts.\n",
    " #example: how many times an adj is followed by a noun {Noun:{Adj:3}}, here its 3 times.\n",
    "\n",
    "#Calculation of the probabilities of tag bigrams for transition probability\n",
    "#We already made a bigram assumption  \n",
    "#Also note that since we are also considering $, the $ row of transition probability matrix give us the initial probabilities as well\n",
    "bigram_tag_prob={}\n",
    "for key in bigram_tag_data.keys():\n",
    "  bigram_tag_prob[key]={}\n",
    "  count=sum(bigram_tag_data[key].values())              # count is total number of times a 'TAG' has occured\n",
    "  for key2 in bigram_tag_data[key].keys():\n",
    "    bigram_tag_prob[key][key2]=bigram_tag_data[key][key2]/count\n",
    "#Tranmission probability is #times a TAG2 is preceded by TAG1 / total number of times TAG1 exists in dataset\n",
    "#example: number of times a noun occured before adjective / total number of times a noun occurred\n",
    " #Calculation the possible tags for each word in the train dataset\n",
    "tags_of_tokens = {}\n",
    "count=0\n",
    "for sent in train_dataset:\n",
    "  for (word,tag) in sent:\n",
    "    word=word.lower()\n",
    "    try:\n",
    "      if tag not in tags_of_tokens[word]:\n",
    "        tags_of_tokens[word].append(tag)\n",
    "    except:\n",
    "      list_of_tags = []\n",
    "      list_of_tags.append(tag)\n",
    "      tags_of_tokens[word] = list_of_tags\n",
    "#Each word and its corresponding tags in the train dataset\n",
    "# Getting words and their corresponding tags from the test set\n",
    "# Seperating the test data into test words and test tags\n",
    "test_words=[]\n",
    "test_tags=[]\n",
    "for sent in test_dataset:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (word,tag) in sent:\n",
    "    temp_word.append(word.lower()) # words of a sentence in test dataset\n",
    "    temp_tag.append(tag) # tags of a sentence in test dataset\n",
    "  test_words.append(temp_word) # list with words of a sentence(tokenized sentence) appended to a list of list\n",
    "  test_tags.append(temp_tag) # list with tags of a sentence(tokenized sentence) appended to a list of list\n",
    " #VITERBI ALGORITHM IMPLEMENTATION\n",
    "# For each word in a sentence\n",
    "# – The probability of best candidates for each tag at previous level is\n",
    "#   multiplied with the emission probability and the transition\n",
    "#   probability of possible tags based on current word\n",
    "# – The best candidate for each tag at the current level is chosen and\n",
    "#   the previous tag is kept a track of for backtracking\n",
    "# – The result of forward propagation at each level looks like the\n",
    "#   following\n",
    "#   LEVEL_K:{TAG1:{best_candidate_among_Tag1,probability_of the\n",
    "#   best candidate} , {TAG2:{previous_tag_of_best_candidate_\n",
    "#   among_Tag2, probability_of the best candidate}, ...}\n",
    "# – When backtracking start from the end to start choosing the\n",
    "#   predicted tag based on LEVEL information and the predicted tag\n",
    "#   that follows the word.\n",
    "# – In case an unseen word arrives the next tag is set to ‘NOUN’ and\n",
    "#   the emission probability is set to a low probability (0.0001). This is\n",
    "#   based on the observation from the corpus that ≈ 63% of the\n",
    "#   unseen words were noun.\n",
    "predicted_tags = []                #Final list for prediction\n",
    "for i in range(len(test_words)):   # for each tokenized sentence in the test data (test_words is a list of lists)\n",
    "  sent = test_words[i]\n",
    "  #storing_values is a dictionary which stores the required values\n",
    "  #ex: storing_values = {step_no.:{state1:[previous_best_state,value_of_the_state]}}                \n",
    "  storing_values = {}              \n",
    "  for q in range(len(sent)):\n",
    "    step = sent[q]\n",
    "    #for the starting word of the sentence\n",
    "    if q == 1:                \n",
    "      storing_values[q] = {}\n",
    "      try:\n",
    "        tags = tags_of_tokens[step]\n",
    "      except:\n",
    "        # print(step,test_tags_of_tokens[step])\n",
    "        tags=[-1] #tags_of_unseen_tokens\n",
    "      for t in tags:\n",
    "        #this is applied since we do not know whether the word in the test data is present in train data or not\n",
    "        try:\n",
    "          storing_values[q][t] = ['^^',bigram_tag_prob['^^'][t]*train_emission_prob[t][step]]\n",
    "        #if word is not present in the train data but present in test data we assign a very low probability of 0.0001\n",
    "        except:\n",
    "          storing_values[q][t] = ['^^',0.0001]\n",
    "    \n",
    "    #if the word is not at the start of the sentence\n",
    "    if q>1:\n",
    "      storing_values[q] = {}\n",
    "      previous_states = list(storing_values[q-1].keys())   # loading the previous states\n",
    "      try:\n",
    "        current_states  = tags_of_tokens[step]               # loading the current states\n",
    "      except:\n",
    "        current_states = [-1]#tags_of_unseen_tokens\n",
    "      #calculation of the best previous state for each current state and then storing\n",
    "      #it in storing_values\n",
    "      for t in current_states:                             \n",
    "        temp = []\n",
    "        for pt in previous_states:                         \n",
    "          try:\n",
    "            temp.append(storing_values[q-1][pt][1]*bigram_tag_prob[pt][t]*train_emission_prob[t][step]) # If seen word\n",
    "          except:\n",
    "            temp.append(storing_values[q-1][pt][1]*0.0001)\n",
    "        max_temp_index = temp.index(max(temp))\n",
    "        best_pt = previous_states[max_temp_index]\n",
    "        storing_values[q][t]=[best_pt,max(temp)] #Store the best previous tag for each best candidate per tag and the meximum probability\n",
    "    #Backtracing to extract the best possible tags for the sentence\n",
    "    # for each word looking the current word and the word and tag next to it in the sentence backtrack\n",
    "    # to get the tag of current word\n",
    "  pred_tags = [] #predicted tags by viterbi using backtracking\n",
    "  total_steps_num = storing_values.keys()\n",
    "  last_step_num = max(total_steps_num)     # Begin from the last word which will end the delimiter\n",
    "  for bs in range(len(total_steps_num)):    \n",
    "    step_num = last_step_num - bs          \n",
    "    if step_num == last_step_num:\n",
    "      pred_tags.append('$$')\n",
    "      pred_tags.append(storing_values[step_num]['$$'][0]) \n",
    "    if step_num<last_step_num and step_num>0:\n",
    "      pred_tags.append(storing_values[step_num][pred_tags[len(pred_tags)-1]][0]) #Looking into storing value fetch the best previous tag for the current word\n",
    "  predicted_tags.append(list(reversed(pred_tags)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4416a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12707/4004271271.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muniq_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muniq_tag_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"^^\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"$$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Exclude delimiters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_tags' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Now that the tags are predicted, get the actual and predicted tags so that analysis can be done\n",
    "tag_seq_act=[]\n",
    "tag_seq_pred=[]\n",
    "uniq_tag=set()\n",
    "uniq_tag_dict={}\n",
    "for li in test_tags:\n",
    "    for tag in li:\n",
    "        if(tag!=\"^^\" and tag!=\"$$\"): #Exclude delimiters\n",
    "          tag_seq_act.append(tag)\n",
    "\n",
    "for li in predicted_tags:\n",
    "  for tag in li:\n",
    "        if(tag!=\"^^\" and tag!=\"$$\"): #Exclude delimiters\n",
    "          tag_seq_pred.append(tag)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1782af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in tag_seq_act:\n",
    "    uniq_tag.add(tag)\n",
    "    \n",
    "for tag in tag_seq_pred:\n",
    "    if tag not in uniq_tag:\n",
    "        uniq_tag.add(tag)\n",
    "    \n",
    "uniq_tag=list(uniq_tag)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bcfc05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c743c131",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniq_tag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12707/889296974.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniq_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0muniq_tag_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muniq_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_seq_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtag_seq_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniq_tag_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uniq_tag' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(uniq_tag)):\n",
    "    uniq_tag_dict[uniq_tag[i]]=i\n",
    "            \n",
    "for i,tag in enumerate(tag_seq_act):\n",
    "    tag_seq_act[i]=uniq_tag_dict[tag]\n",
    "\n",
    "for i,tag in enumerate(tag_seq_pred):\n",
    "    tag_seq_pred[i]=uniq_tag_dict[tag]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ab751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e418762",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tag_seq_act' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12707/3075596692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the precision, Recall and F-Scores by comparing the actual and predicted tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatched_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_seq_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtag_seq_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtag_seq_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mmatched_tags\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tag_seq_act' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the precision, Recall and F-Scores by comparing the actual and predicted tags\n",
    "matched_tags=0\n",
    "for i in range(len(tag_seq_act)):\n",
    "    if tag_seq_act[i]==tag_seq_pred[i]:\n",
    "      matched_tags+=1\n",
    "  # Estimations for the current set\n",
    "precision=matched_tags/(len(tag_seq_pred))\n",
    "recall=matched_tags/(len(tag_seq_act))\n",
    "F1_score=(2*precision*recall)/(precision+recall)\n",
    "F05_score=(1.25*precision*recall)/(0.25*precision+recall)\n",
    "F2_score=(5*precision*recall)/(4*precision+recall)\n",
    "# A confusion matrix is created which is used to diplay the confusion matrix and for per pos evaluation\n",
    "conf_matrix=confusion_matrix(tag_seq_act,tag_seq_pred)\n",
    "# pos_estimation=per_POS_evaluation(conf_matrix, uniq_tag)\n",
    "\n",
    "#Store every set's estimations\n",
    "precision_sets[setno]=precision\n",
    "recall_sets[setno]=recall\n",
    "F1_score_sets[setno]=F1_score\n",
    "F05_score_sets[setno]=F05_score\n",
    "F2_score_sets[setno]=F2_score\n",
    "# pos_estimation_sets[setno]=pos_estimation\n",
    "print(\"Set \",setno+1,\"✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f68f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "OVERALL ESTIMATIONS\n",
      "===================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_sets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12707/3935119501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===================\\nOVERALL ESTIMATIONS\\n===================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overall Precision:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"±\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetno\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overall Recall:\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"±\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetno\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overall F1 Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1_score_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"±\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1_score_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetno\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overall F0.5 Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1_score_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"±\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{:.6f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1_score_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetno\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_sets' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"===================\\nOVERALL ESTIMATIONS\\n===================\")\n",
    "print(\"Overall Precision:\", \"{:.6f}\".format(statistics.mean(precision_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(precision_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall Recall:\" ,\"{:.6f}\".format(statistics.mean(recall_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(recall_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall F1 Score:\", \"{:.6f}\".format(statistics.mean(F1_score_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(F1_score_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall F0.5 Score:\", \"{:.6f}\".format(statistics.mean(F1_score_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(F1_score_sets)/math.sqrt(setno+1)))\n",
    "print(\"Overall F2 Score:\", \"{:.6f}\".format(statistics.mean(F1_score_sets)),\"±\",\"{:.6f}\".format(statistics.stdev(F1_score_sets)/math.sqrt(setno+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c6c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ca403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c6fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009ba5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02592cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fdcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
