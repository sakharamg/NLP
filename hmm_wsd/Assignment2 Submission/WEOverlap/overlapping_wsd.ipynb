{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WSD_Improved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZcSZmL-ON2T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITTFGLqxfAw6",
        "outputId": "4f31d539-0cc6-4f96-86c6-14dc9807bbdc"
      },
      "source": [
        "!pip3 install nltk==3.6.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.6.2\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUXnj6GjeR07",
        "outputId": "3e4db48f-1a64-4809-9d2c-5c15f19d1b0e"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import semcor\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Getting the brown corpus\n",
        "print(\"Downloading files from NLTK please wait...\")\n",
        "nltk.download('all', quiet=True)\n",
        "print(\"NLTK files downloaded!\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files from NLTK please wait...\n",
            "NLTK files downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEpA53bfMi1q"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEqjRWWY0va7"
      },
      "source": [
        "dict={}\n",
        "for sentence in semcor.tagged_sents(tag='pos'):\n",
        "  for word in sentence:\n",
        "    dict[word.label()]=1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHIvcL1uOV5x",
        "outputId": "3c3a7b55-7dcf-4f4b-99b3-04be25e86112"
      },
      "source": [
        "dict.keys()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['DT', 'NNP', 'VB', 'NN', 'IN', 'POS', 'JJ', None, 'RB', 'WDT', 'CC', 'VBD', 'VBN', 'TO', 'PRP', 'MD', 'VBZ', 'PRP$', 'WRB', 'CD', 'EX', 'VBP', 'WP', 'NNS', 'VBG', 'MD|VB', 'NNPS', 'PDT', 'UH', 'WP$', 'LS', 'FW', 'NPS', 'JJR', 'RBR', 'PP', 'RBS', 'NP', 'RP', 'PR', 'JJS', 'NNP|NP', 'NNP|VBN', 'NN|SYM'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IhWjMfu0sX0"
      },
      "source": [
        "synCorpus=[]\n",
        "current_index=0\n",
        "for taggedSents,postaggedSents in zip(semcor.tagged_sents(tag='sem'), semcor.tagged_sents(tag='pos')): # for each sentence fetch the (phrase,synsetTree)\n",
        "    synCorpus.append([])\n",
        "    # synposCorpus.append([])                           # create a list designating current sentence which will contain (phrase,synsetID)\n",
        "    for phrase,posphrase in zip(taggedSents,postaggedSents):\n",
        "        if type(phrase)==list:                     # The phrase with no tag is a list in semcor [Note: CASE 1]\n",
        "            pass\n",
        "        else: \n",
        "            try: \n",
        "                # handle Case 2\n",
        "                synCorpus[current_index].append((phrase.label().name(),phrase.label().synset().name(),posphrase.label())) \n",
        "            except:\n",
        "                # handle Case 3, The tags with no synset id but a synset present\n",
        "                pass\n",
        "    current_index+=1 # Keeps track of the sentence index\n",
        "no_sents=current_index #Saved total number of sentences"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njudl5KOfjmy",
        "outputId": "112e45b6-ffc0-40b4-8eed-c4d228adc680"
      },
      "source": [
        "print(\"Loading word2vec\")\n",
        "!wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "try:\n",
        "\tmodel_w2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
        "except:\n",
        "\tprint(\"Download pretrained word2vec from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz and save in the same directory of the file\\nExiting...\")\n",
        "\texit()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading word2vec\n",
            "--2021-11-04 09:56:43--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.11.70\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.11.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  83.9MB/s    in 19s     \n",
            "\n",
            "2021-11-04 09:57:02 (84.4 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5UzkH3Nf6WP",
        "outputId": "18c09fed-3349-4fa5-8744-fa04aec37a79"
      },
      "source": [
        "import numpy as np\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "print(\"Testing and getting accuracy estimate\")\n",
        "synCorpus_words=[]\n",
        "synCorpus_pos=[]\n",
        "line=0\n",
        "\n",
        "# Get all the sentences in corpus\n",
        "for sent in synCorpus:\n",
        "    synCorpus_words.append([])\n",
        "    synCorpus_pos.append([])\n",
        "    for (word,tag,postag) in sent:\n",
        "        synCorpus_words[line].append(word)\n",
        "        synCorpus_pos[line].append(postag)\n",
        "    line+=1\n",
        "\n",
        "# Find context bag for each sentence, the ambiguous word is ignored when comparing with the ambigous word's sense\n",
        "length_of_wordvec=len(model_w2v[\"the\"])\n",
        "context_bag_vector = []\n",
        "for sent in synCorpus_words:\n",
        "    context_count=0\n",
        "    context_list=np.zeros((length_of_wordvec,))\n",
        "    #for every sentence get the context bag\n",
        "    single_sentence='_'.join(word for word in sent)\n",
        "    single_sentence=single_sentence.replace('-','_')\n",
        "    splits=single_sentence.split('_')\n",
        "    for word in splits:\n",
        "        try:\n",
        "            context_list+=model_w2v[word]\n",
        "            context_count+=1\n",
        "        except:\n",
        "            pass\n",
        "    if context_count!=0:\n",
        "        context_bag_vector.append(context_list/context_count)\n",
        "    else:\n",
        "        context_bag_vector.append(context_list)\n",
        "\n",
        "\n",
        "# Find similarity with sense bag for each sense\n",
        "overlapCorpus=[]\n",
        "count=0\n",
        "test=0\n",
        "test1=0\n",
        "for sent,posSent in zip(synCorpus_words,synCorpus_pos):\n",
        "    sentence=\"\"\n",
        "    pos_tag_sentence=\"\"\n",
        "    for word in sent:\n",
        "      sentence+=\" \"+word\n",
        "    # for postag_word in posSent:\n",
        "    #   pos_tag_sentence+=\" \"+postag_word\n",
        "    overlapCorpus.append([])\n",
        "    # sentence_pos_tags=generate_pos_tags(sentence)[0][1:-1]\n",
        "    curr_word_index=-1\n",
        "    for word,pos in zip(sent,posSent):\n",
        "        # print(pos_tag_sentence)\n",
        "        curr_word_index+=1\n",
        "        synsets_word=wn.synsets(word)\n",
        "        if len(synsets_word) ==0:\n",
        "          overlapCorpus[count].append((word,'NOT IN WORDNET'))\n",
        "          continue\n",
        "        else:\n",
        "          # print(pos)\n",
        "          if 'NN' in pos:\n",
        "            if len(wn.synsets(word,pos=wn.NOUN))!=0:\n",
        "              synsets_word=wn.synsets(word,pos=wn.NOUN)\n",
        "          elif 'JJ' in pos:\n",
        "            if len(wn.synsets(word,pos=wn.ADJ))!=0:\n",
        "              synsets_word=wn.synsets(word,pos=wn.ADJ)\n",
        "          elif 'RB' in pos:\n",
        "            if len(wn.synsets(word,pos=wn.ADV))!=0:\n",
        "              synsets_word=wn.synsets(word,pos=wn.ADV)\n",
        "          elif 'VB' in pos:\n",
        "            if len(wn.synsets(word,pos=wn.VERB))!=0:\n",
        "              synsets_word=wn.synsets(word,pos=wn.VERB)\n",
        "          else:\n",
        "            pass\n",
        "        curr_sim=-1\n",
        "        try:\n",
        "          curr_synset=synsets_word[0]\n",
        "        except:\n",
        "          pass\n",
        "        for synset_word in synsets_word: # for each sense of a word\n",
        "            amb_list=np.zeros((length_of_wordvec,))\n",
        "            amb_count=0\n",
        "            synset_gloss=synset_word.definition() # get the word's gloss\n",
        "            all_eg=synset_word.examples()\n",
        "            synset_all_eg=\"\"\n",
        "            for i in range(len(all_eg)):\n",
        "              if all_eg[i]==' ':\n",
        "                break\n",
        "              synset_all_eg+=\" \"+all_eg[i]\n",
        "            synset_gloss+=\" \"+synset_all_eg\n",
        "            splits=synset_gloss.split()\n",
        "            for word1 in splits:\n",
        "                try:\n",
        "                    if(word1!=word):\n",
        "                        amb_list+=model_w2v[word1]\n",
        "                        amb_count+=1\n",
        "                except:\n",
        "                    pass\n",
        "            if amb_count!=0: # for context bag wrt an ambiguous word\n",
        "                amb_vec=(amb_list/amb_count)\n",
        "            else:\n",
        "                amb_vec=amb_list\n",
        "            cos_similarity=model_w2v.cosine_similarities(context_bag_vector[count].T,[amb_vec.T])[0] # Check cosine similary\n",
        "            if curr_sim<cos_similarity: # and update the sense to the most similar sense\n",
        "                curr_sim=cos_similarity\n",
        "                curr_synset=synset_word\n",
        "        overlapCorpus[count].append((word,curr_synset.name()))\n",
        "    count+=1\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing and getting accuracy estimate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUpfuv2dVEWP",
        "outputId": "71de22db-0906-4650-d17d-7350abfb1925"
      },
      "source": [
        "# Test the accuracy of overlap based\n",
        "total_count=0\n",
        "correct_count=0\n",
        "current_sent=0\n",
        "for sent in synCorpus:\n",
        "    current_word=0\n",
        "    for (word,tag,pos) in sent:\n",
        "        if tag==overlapCorpus[current_sent][current_word][1]:\n",
        "            correct_count+=1\n",
        "        total_count+=1\n",
        "        current_word+=1\n",
        "    current_sent+=1    \n",
        "print(\"Accuracy of WSD with overlapping: \",correct_count*100/total_count,'%')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of WSD with overlapping:  48.386407732426704 %\n"
          ]
        }
      ]
    }
  ]
}